{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22847d5-0aae-400f-8be8-143a8299fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0760457-941c-46a9-adaf-690e7c7f85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the HTML content of a page\n",
    "def get_html(url):\n",
    "    # Create a session to handle cookies and headers\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Fetch the content of the page\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    \n",
    "    else:\n",
    "        print(f'Failed to retrieve the page. Status code: {response.status_code}')\n",
    "\n",
    "\n",
    "# Function to parse the given page and extract the first 200 book titles and links\n",
    "def parse_main_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    books = [book for book in soup.find_all('li') if book.get(\"class\") == [\"pgdbetext\"]] #Filter out non-book content\n",
    "    \n",
    "    book_data = []\n",
    "    for book in books:\n",
    "        link_tag = book.find('a')\n",
    "        if link_tag:\n",
    "            title = link_tag.text.strip()\n",
    "            link = 'https://www.gutenberg.org' + link_tag['href']\n",
    "            book_data.append((title, link))\n",
    "    \n",
    "        if len(book_data) == 200 : break\n",
    "    return book_data\n",
    "\n",
    "# Function to save the data to a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Title', 'URL'])\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Function to read the CSV file and extract links\n",
    "def read_csv_and_get_links(filename):\n",
    "    links = []\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            if len(row) > 1:  # Ensure the row has enough columns\n",
    "                links.append(row[1])  # The URL is in the second column\n",
    "    return links\n",
    "\n",
    "#--------Functions for extracting data from individual books -----------------------\n",
    "\n",
    "def  get_author(soup):\n",
    "    try:\n",
    "        author = soup.find('a', itemprop = \"creator\").text.strip()\n",
    "    except:\n",
    "        print(\"No author found\")\n",
    "        author = \"\"\n",
    "    return author\n",
    "    \n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('td', itemprop = \"headline\").text.strip()\n",
    "    except:\n",
    "        print(\"Did not find title\")\n",
    "        title = \"\"\n",
    "    return title\n",
    "\n",
    "#Get publish date of book\n",
    "def get_pub_date(soup):\n",
    "    try:\n",
    "        pub_date = soup.find(\"td\", itemprop = \"datePublished\").text.strip()\n",
    "    except:\n",
    "        print(\"No date found\")\n",
    "        pub_date = \"\"\n",
    "    return pub_date\n",
    "\n",
    "#Extract book content from web version e-book\n",
    "def extract_book_content(content_url):\n",
    "    content_html = get_html(content_url)\n",
    "    soup = BeautifulSoup(content_html)\n",
    "    \n",
    "    # Locate the start and end markers\n",
    "    start_marker = soup.find(\"span\", string=lambda text: text and \"START OF THE PROJECT GUTENBERG EBOOK\" in text)\n",
    "    end_marker = soup.find(\"span\", string=lambda text: text and \"END OF THE PROJECT GUTENBERG EBOOK\" in text)\n",
    "\n",
    "    if start_marker and end_marker:\n",
    "        # Find the elements that contain the markers\n",
    "        start_element = start_marker.find_parent()\n",
    "        end_element = end_marker.find_parent()\n",
    "        \n",
    "        # Extract all text between the start and end elements\n",
    "        content = []\n",
    "        for element in start_element.find_all_next(string=True):\n",
    "            if start_marker.text in element:\n",
    "                continue\n",
    "            if end_marker.text in element:\n",
    "                break\n",
    "            # Filter out whitespace and empty strings\n",
    "            if element.strip():\n",
    "                content.append(element.strip())\n",
    "\n",
    "        return \"\\n\".join(content)\n",
    "    \n",
    "    else:\n",
    "        print(\"Could not find the book markers in the text.\")\n",
    "        return None\n",
    "\n",
    "#Get book data (including title, author, publish date + book content)\n",
    "def get_book_data(book_url):\n",
    "    book_html = get_html(book_url)\n",
    "    soup = BeautifulSoup(book_html)\n",
    "\n",
    "    #Get basic info (title, author, publish date)\n",
    "    book_info = {}\n",
    "    book_info[\"title\"], book_info[\"author\"], book_info[\"date\"] = get_title(soup), get_author(soup), get_pub_date(soup)\n",
    "\n",
    "    #Get read online url\n",
    "    content_url = \"https://www.gutenberg.org\"+soup.find(\"a\", title = \"Read online\").get(\"href\")\n",
    "    #Extract content of book\n",
    "    book_content = extract_book_content(content_url)\n",
    "    \n",
    "    return book_info, book_content #Returns a dictionary containing book info (keys: title, author, date) and a string containing the content of the book\n",
    "\n",
    "def save_to_txt(book_info, book_content):\n",
    "    title = book_info[\"title\"].replace(\"/\", \",\") #If '/' exists in book title, replace with ','\n",
    "    \n",
    "    # Format the book info\n",
    "    info = (f\"書名: {title}\\n\"\n",
    "            f\"作者: {book_info['author']}\\n\"\n",
    "            f\"時間: {book_info['date']}\\n\\n\")\n",
    "    \n",
    "    full_text = info + book_content\n",
    "\n",
    "    # Write to a text file with book title as file name, save to folder \"top_200_books\"\n",
    "    if not os.path.exists('top_200_books'):\n",
    "        os.makedirs('top_200_books')\n",
    "\n",
    "    file_name = f'top_200_books/{title}.txt'\n",
    "    duplicate_num = 0 \n",
    "\n",
    "    while os.path.exists(file_name): #Check if title already exists\n",
    "        duplicate_num += 1\n",
    "        file_name = f'top_200_books/{title}_{duplicate_num}.txt' #Alternative title for duplicates (eg. 比目魚_1.txt) \n",
    "        \n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(full_text)\n",
    "    \n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44ebdc-4e7e-4547-bb91-10ec73675cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Crawl book links for the first 200 books\n",
    "    main_page_url = \"https://www.gutenberg.org/browse/languages/zh\"\n",
    "    main_page_html = get_html(main_page_url)\n",
    "    \n",
    "    top_200_data = parse_main_page(main_page_html)\n",
    "    \n",
    "    save_to_csv(top_200_data, \"top_200_links.csv\")\n",
    "    \n",
    "    #Retrieve the 200 links crawled\n",
    "    links = read_csv_and_get_links(\"top_200_links.csv\")\n",
    "    \n",
    "    #Crawl each book and save to txt file\n",
    "    problem_links = [] \n",
    "    link_index = 0\n",
    "    for link in links:\n",
    "        try:\n",
    "            info, content = get_book_data(link)\n",
    "            save_to_txt(info, content)\n",
    "        except:\n",
    "            print(f\"Problem with {link_index}\")\n",
    "            problem_links.append(link)\n",
    "        link_index+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
